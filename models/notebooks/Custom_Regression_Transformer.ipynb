{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "386b1238",
   "metadata": {},
   "source": [
    "# Regression Transformer Model\n",
    "Predicts the number of comments an article will get based on the article's topic, tone, length, hour of the day and day of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "184f4835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Layer\n",
    "from keras.layers import Embedding, Input, GlobalAveragePooling1D, Dense, Flatten\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential, Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b02a2f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_block(embed_dim, num_heads, ff_dim, dropout_rate):\n",
    "    att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "    ffn = Sequential(\n",
    "        [Dense(ff_dim, activation=\"relu\"),\n",
    "         Dense(embed_dim), ]\n",
    "    )\n",
    "    layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "    layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "    dropout1 = Dropout(dropout_rate)\n",
    "    dropout2 = Dropout(dropout_rate)\n",
    "    \n",
    "    def call(inputs, training):\n",
    "        attn_output = att(inputs, inputs)\n",
    "        attn_output = dropout1(attn_output, training=training)\n",
    "        out1 = layernorm1(inputs + attn_output)\n",
    "        ffn_output = ffn(out1)\n",
    "        ffn_output = dropout2(ffn_output, training=training)\n",
    "        return layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    return call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fa92a32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load training data\n",
    "train_df = pd.read_csv('../data/regression_train_data.csv')\n",
    "\n",
    "# Extract the columns you want to use as input features\n",
    "columns = ['BERT_sentiment_score', 'normalised_word_count', 'pub_day', 'pub_hour', 'topic']\n",
    "# Extract input features from the dataframe\n",
    "x_train = np.array(train_df[columns])\n",
    "\n",
    "y_train = np.array(train_df['n_comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "28b84ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "30867f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_mae', \n",
    "                               # Number of epochs to wait for improvement\n",
    "                               patience=10,  \n",
    "                               verbose=1, \n",
    "                               # Restore the weights of the best epoch\n",
    "                               restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6bcba855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypreparameters\n",
    "max_seq_length = x_train.shape[1]\n",
    "vocab_size = int(np.max(x_train) + 1)\n",
    "embedding_dim = 32\n",
    "num_heads = 2\n",
    "ff_dim = 32\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5e8b2517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 5)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 5, 32)        2688        ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 5, 32)       8416        ['embedding_2[0][0]',            \n",
      " eadAttention)                                                    'embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 5, 32)        0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 5, 32)       0           ['embedding_2[0][0]',            \n",
      " mbda)                                                            'dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 5, 32)       64          ['tf.__operators__.add_4[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " sequential_2 (Sequential)      (None, 5, 32)        2112        ['layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 5, 32)        0           ['sequential_2[0][0]']           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 5, 32)       0           ['layer_normalization_4[0][0]',  \n",
      " mbda)                                                            'dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 5, 32)       64          ['tf.__operators__.add_5[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2 (Gl  (None, 32)          0           ['layer_normalization_5[0][0]']  \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 32)           0           ['global_average_pooling1d_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 1)            33          ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 13,377\n",
      "Trainable params: 13,377\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define model architecture\n",
    "\n",
    "# Input\n",
    "inputs = Input(shape=(max_seq_length,))\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_seq_length)(inputs)\n",
    "\n",
    "# Hidden layers\n",
    "transformer_block_fn = transformer_block(embed_dim=embedding_dim, \n",
    "                                         num_heads=num_heads, ff_dim=ff_dim, \n",
    "                                         dropout_rate=dropout_rate)\n",
    "transformer_block = transformer_block_fn(embedding_layer, training=True)\n",
    "pooling_layer = GlobalAveragePooling1D()(transformer_block)\n",
    "dropout_layer = Dropout(rate=0.1)(pooling_layer)\n",
    "\n",
    "# Output\n",
    "outputs = Dense(units=1, activation='linear')(dropout_layer)  # Change activation to 'linear' for regression\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7d7031fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "125/125 [==============================] - 2s 9ms/step - loss: 373938.5938 - mae: 302.9965 - val_loss: 321037.4375 - val_mae: 274.5068\n",
      "Epoch 2/100\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 368464.0000 - mae: 297.4241 - val_loss: 315376.8125 - val_mae: 269.0971\n",
      "Epoch 3/100\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 361220.4688 - mae: 292.0836 - val_loss: 307846.7812 - val_mae: 264.0556\n",
      "Epoch 4/100\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 352247.7500 - mae: 287.1567 - val_loss: 299199.7188 - val_mae: 260.3413\n",
      "Epoch 5/100\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 342388.6562 - mae: 283.8164 - val_loss: 290011.8125 - val_mae: 258.2507\n",
      "Epoch 6/100\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 332039.6562 - mae: 281.6322 - val_loss: 280753.1562 - val_mae: 257.5047\n",
      "Epoch 7/100\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 321522.1250 - mae: 277.8135 - val_loss: 271278.5938 - val_mae: 253.0951\n",
      "Epoch 8/100\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 310888.1875 - mae: 274.6808 - val_loss: 262297.6875 - val_mae: 250.7298\n",
      "Epoch 9/100\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 300883.0000 - mae: 273.1589 - val_loss: 254147.9219 - val_mae: 250.0720\n",
      "Epoch 10/100\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 291909.2812 - mae: 272.7878 - val_loss: 246609.6875 - val_mae: 248.3431\n",
      "Epoch 11/100\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 281804.2188 - mae: 264.3396 - val_loss: 237911.5156 - val_mae: 243.6503\n",
      "Epoch 12/100\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 272440.7188 - mae: 261.0250 - val_loss: 230365.3906 - val_mae: 240.4255\n",
      "Epoch 13/100\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 263323.8125 - mae: 258.5562 - val_loss: 223680.4531 - val_mae: 245.0058\n",
      "Epoch 14/100\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 255760.3750 - mae: 255.8167 - val_loss: 217841.7188 - val_mae: 231.5957\n",
      "Epoch 15/100\n",
      "125/125 [==============================] - 1s 9ms/step - loss: 247893.6406 - mae: 254.0827 - val_loss: 210529.6406 - val_mae: 235.2888\n",
      "Epoch 16/100\n",
      "125/125 [==============================] - 1s 9ms/step - loss: 240339.5781 - mae: 252.2903 - val_loss: 204921.7344 - val_mae: 231.8196\n",
      "Epoch 17/100\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 234220.9375 - mae: 252.3392 - val_loss: 201326.1406 - val_mae: 242.7894\n",
      "Epoch 18/100\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 228571.6094 - mae: 251.1659 - val_loss: 195339.7812 - val_mae: 237.8768\n",
      "Epoch 19/100\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 224075.0312 - mae: 249.2088 - val_loss: 191817.6875 - val_mae: 230.6945\n",
      "Epoch 20/100\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 219565.0156 - mae: 248.2926 - val_loss: 186747.9219 - val_mae: 224.7205\n",
      "Epoch 21/100\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 214959.0625 - mae: 246.9346 - val_loss: 183710.4062 - val_mae: 229.7209\n",
      "Epoch 22/100\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 211440.6562 - mae: 246.9569 - val_loss: 179762.1875 - val_mae: 227.2262\n",
      "Epoch 23/100\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 208557.7969 - mae: 246.6024 - val_loss: 178020.6406 - val_mae: 227.8131\n",
      "Epoch 24/100\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 205939.3281 - mae: 246.2328 - val_loss: 176216.0469 - val_mae: 230.3701\n",
      "Epoch 25/100\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 204652.4844 - mae: 246.6819 - val_loss: 176241.2812 - val_mae: 236.3704\n",
      "Epoch 26/100\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 201105.7656 - mae: 245.5266 - val_loss: 174051.2031 - val_mae: 223.0956\n",
      "Epoch 27/100\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 199566.7031 - mae: 243.9081 - val_loss: 173609.7188 - val_mae: 234.7597\n",
      "Epoch 28/100\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 198484.1094 - mae: 244.6315 - val_loss: 173254.4688 - val_mae: 237.8148\n",
      "Epoch 29/100\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 195897.2969 - mae: 243.6724 - val_loss: 170949.5000 - val_mae: 225.1346\n",
      "Epoch 30/100\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 194173.3906 - mae: 243.1422 - val_loss: 170416.6094 - val_mae: 226.1271\n",
      "Epoch 31/100\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 194418.9375 - mae: 244.2314 - val_loss: 171068.1875 - val_mae: 234.3294\n",
      "Epoch 32/100\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 193518.7969 - mae: 244.2177 - val_loss: 170605.1562 - val_mae: 223.1261\n",
      "Epoch 33/100\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 192904.2031 - mae: 243.3267 - val_loss: 169702.1094 - val_mae: 226.7552\n",
      "Epoch 34/100\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 191696.9844 - mae: 243.6092 - val_loss: 169823.1406 - val_mae: 228.9801\n",
      "Epoch 35/100\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 191083.4219 - mae: 243.1482 - val_loss: 169045.8125 - val_mae: 224.7001\n",
      "Epoch 36/100\n",
      "122/125 [============================>.] - ETA: 0s - loss: 190235.0156 - mae: 242.8452Restoring model weights from the end of the best epoch: 26.\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 189565.3438 - mae: 242.6617 - val_loss: 169773.1875 - val_mae: 228.5927\n",
      "Epoch 36: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d7e90bdfd0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='mse', metrics=['mae'])  # Change loss function to 'mse' for regression\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=100, validation_data=(x_val, y_val), \n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "585304a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained model\n",
    "model.save('../models/transformer_regression_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9ea41546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step - loss: 173913.9219 - mae: 223.2462\n",
      "Validation MSE: 173913.921875\n",
      "Validation MAE: 223.24618530273438\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on validation data\n",
    "mse, mae = model.evaluate(x_val, y_val)\n",
    "print('Validation MSE:', mse)\n",
    "print('Validation MAE:', mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c352147",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
