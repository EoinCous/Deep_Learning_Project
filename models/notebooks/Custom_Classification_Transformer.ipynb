{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0c76b28",
   "metadata": {},
   "source": [
    "# Classification Transformer Model\n",
    "Finds the tone of an article based on the article's headline, abstract and key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9764e680",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c923e215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Layer\n",
    "from keras.layers import Embedding, Input, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb2ec081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_block(embed_dim, num_heads, ff_dim, dropout_rate):\n",
    "    att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "    ffn = Sequential(\n",
    "        [Dense(ff_dim, activation=\"relu\"),\n",
    "         Dense(embed_dim), ]\n",
    "    )\n",
    "    layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "    layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "    dropout1 = Dropout(dropout_rate)\n",
    "    dropout2 = Dropout(dropout_rate)\n",
    "    \n",
    "    def call(inputs, training):\n",
    "        attn_output = att(inputs, inputs)\n",
    "        attn_output = dropout1(attn_output, training=training)\n",
    "        out1 = layernorm1(inputs + attn_output)\n",
    "        ffn_output = ffn(out1)\n",
    "        ffn_output = dropout2(ffn_output, training=training)\n",
    "        return layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    return call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61e43b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train_df = pd.read_csv('../data/cleaned_train.csv')\n",
    "x_train = train_df['numerical_sequence'].apply(lambda x: [int(i) for i in x.split(',')])\n",
    "y_train = np.array(train_df['BERT_sentiment_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dce68a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_df = pd.read_csv('../data/cleaned_test.csv')\n",
    "x_test = np.array(test_df['numerical_sequence'].apply(lambda x: [int(i) for i in x.split(',')]))\n",
    "y_test = np.array(test_df['BERT_sentiment_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32b23201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the longest sequence\n",
    "max_seq_length = max([len(seq) for seq in x_test] + [len(seq) for seq in x_train])\n",
    "\n",
    "# Pad sequences with zeros to have the same length\n",
    "x_train = pad_sequences(x_train, maxlen= max_seq_length, padding='post', truncating='post')\n",
    "x_test = pad_sequences(x_test, maxlen= max_seq_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bba5afcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0143a715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', \n",
    "                               # Number of epochs to wait for improvement\n",
    "                               patience=3,  \n",
    "                               verbose=1, \n",
    "                               # Restore the weights of the best epoch\n",
    "                               restore_best_weights=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f08237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "# Get the maximum value across all elements in the combined array\n",
    "x_combined = np.concatenate((x_train, x_test, x_val), axis=0)\n",
    "vocab_size = np.max(x_combined) + 1\n",
    "\n",
    "embedding_dim = 16\n",
    "num_heads = 2\n",
    "ff_dim = 32\n",
    "dropout_rate = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "442fe369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 227)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 227, 16)      611472      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 227, 16)     2160        ['embedding[0][0]',              \n",
      " dAttention)                                                      'embedding[0][0]']              \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 227, 16)      0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 227, 16)     0           ['embedding[0][0]',              \n",
      " da)                                                              'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 227, 16)     32          ['tf.__operators__.add[0][0]']   \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 227, 16)      1072        ['layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 227, 16)      0           ['sequential[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 227, 16)     0           ['layer_normalization[0][0]',    \n",
      " mbda)                                                            'dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 227, 16)     32          ['tf.__operators__.add_1[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 16)          0           ['layer_normalization_1[0][0]']  \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 16)           0           ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 3)            51          ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 614,819\n",
      "Trainable params: 614,819\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define model architecture\n",
    "\n",
    "# Input \n",
    "inputs = Input(shape=(max_seq_length,))\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(inputs)\n",
    "\n",
    "# Hidden\n",
    "transformer_block_fn = transformer_block(embed_dim=embedding_dim, \n",
    "                                         num_heads=num_heads, \n",
    "                                         ff_dim=ff_dim,\n",
    "                                        dropout_rate=dropout_rate)\n",
    "transformer_block = transformer_block_fn(embedding_layer, training=True)\n",
    "pooling_layer = GlobalAveragePooling1D()(transformer_block)\n",
    "dropout_layer = Dropout(dropout_rate)(pooling_layer)\n",
    "\n",
    "# Output\n",
    "outputs = Dense(units=3, activation='softmax')(dropout_layer)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dd932a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "623/623 [==============================] - 17s 24ms/step - loss: 1.0141 - accuracy: 0.5019 - val_loss: 0.8914 - val_accuracy: 0.5989\n",
      "Epoch 2/10\n",
      "623/623 [==============================] - 15s 24ms/step - loss: 0.7801 - accuracy: 0.6592 - val_loss: 0.8286 - val_accuracy: 0.6370\n",
      "Epoch 3/10\n",
      "623/623 [==============================] - 15s 24ms/step - loss: 0.4996 - accuracy: 0.8060 - val_loss: 0.8665 - val_accuracy: 0.6619\n",
      "Epoch 4/10\n",
      "623/623 [==============================] - 15s 24ms/step - loss: 0.2583 - accuracy: 0.9098 - val_loss: 1.0695 - val_accuracy: 0.6426\n",
      "Epoch 5/10\n",
      "623/623 [==============================] - 15s 24ms/step - loss: 0.1145 - accuracy: 0.9632 - val_loss: 1.4939 - val_accuracy: 0.6362\n",
      "Epoch 6/10\n",
      "622/623 [============================>.] - ETA: 0s - loss: 0.0498 - accuracy: 0.9853Restoring model weights from the end of the best epoch: 3.\n",
      "623/623 [==============================] - 15s 24ms/step - loss: 0.0498 - accuracy: 0.9854 - val_loss: 1.9189 - val_accuracy: 0.6266\n",
      "Epoch 6: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27ca5902ca0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train, y_train, batch_size=16, epochs=10, validation_data=(x_val, y_val),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0a313be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained model\n",
    "model.save('../models/transformer_classification_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f7061ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('../models/transformer_classification_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "99ca71ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8872 - accuracy: 0.6424\n",
      "Validation loss: 0.8872432708740234\n",
      "Validation accuracy: 0.6424272656440735\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on validation data\n",
    "loss, accuracy = model.evaluate(x_val, y_val)\n",
    "print('Validation loss:', loss)\n",
    "print('Validation accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7660750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
