{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30725c24",
   "metadata": {},
   "source": [
    "# Classification Transformer Model\n",
    "Finds the tone of an article based on the article's headline, abstract and key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac397d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c923e215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Layer\n",
    "from keras.layers import Embedding, Input, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bb2ec081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_block(embed_dim, num_heads, ff_dim, dropout_rate):\n",
    "    att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "    ffn = Sequential(\n",
    "        [Dense(ff_dim, activation=\"relu\"),\n",
    "         Dense(embed_dim), ]\n",
    "    )\n",
    "    layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "    layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "    dropout1 = Dropout(dropout_rate)\n",
    "    dropout2 = Dropout(dropout_rate)\n",
    "    \n",
    "    def call(inputs, training):\n",
    "        attn_output = att(inputs, inputs)\n",
    "        attn_output = dropout1(attn_output, training=training)\n",
    "        out1 = layernorm1(inputs + attn_output)\n",
    "        ffn_output = ffn(out1)\n",
    "        ffn_output = dropout2(ffn_output, training=training)\n",
    "        return layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    return call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1f708c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train_df = pd.read_csv('../data/cleaned_train.csv')\n",
    "x_train = np.array(train_df['numerical_sequence'].dropna().apply(lambda x: [int(i) for i in x.split(',')]).tolist())\n",
    "y_train = np.array(train_df['BERT_sentiment_score'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dce68a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_df = pd.read_csv('../data/cleaned_test.csv')\n",
    "x_test = np.array(test_df['numerical_sequence'].apply(lambda x: [int(i) for i in x.split(',')]).tolist())\n",
    "y_test = np.array(test_df['BERT_sentiment_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d8a038a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to a fixed length\n",
    "maxlen = 227 # assuming the maximum sequence length is 200\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen, truncating='post')\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen, truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bba5afcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0143a715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', \n",
    "                               # Number of epochs to wait for improvement\n",
    "                               patience=3,  \n",
    "                               verbose=1, \n",
    "                               # Restore the weights of the best epoch\n",
    "                               restore_best_weights=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "aa8b6f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "max_seq_length = x_train.shape[1]\n",
    "# Concatenate x_train and x_test along the appropriate axis (assuming axis=0)\n",
    "x_combined = np.concatenate((x_train, x_test), axis=0)\n",
    "# Get the maximum value across all elements in the combined array\n",
    "vocab_size = np.max(x_combined) + 1\n",
    "embedding_dim = 16\n",
    "num_heads = 2\n",
    "ff_dim = 32\n",
    "dropout_rate = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6dd932a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "623/623 [==============================] - 18s 26ms/step - loss: 1.1008 - accuracy: 0.4748 - val_loss: 0.9257 - val_accuracy: 0.5860\n",
      "Epoch 2/10\n",
      "623/623 [==============================] - 17s 27ms/step - loss: 0.7905 - accuracy: 0.6672 - val_loss: 0.7763 - val_accuracy: 0.6619\n",
      "Epoch 3/10\n",
      "623/623 [==============================] - 16s 26ms/step - loss: 0.5165 - accuracy: 0.8065 - val_loss: 0.8499 - val_accuracy: 0.6498\n",
      "Epoch 4/10\n",
      "623/623 [==============================] - 16s 25ms/step - loss: 0.2894 - accuracy: 0.9023 - val_loss: 1.1302 - val_accuracy: 0.6334\n",
      "Epoch 5/10\n",
      "623/623 [==============================] - ETA: 0s - loss: 0.1457 - accuracy: 0.9567Restoring model weights from the end of the best epoch: 2.\n",
      "623/623 [==============================] - 16s 25ms/step - loss: 0.1457 - accuracy: 0.9567 - val_loss: 1.3900 - val_accuracy: 0.6101\n",
      "Epoch 5: early stopping\n"
     ]
    }
   ],
   "source": [
    "# define model architecture\n",
    "inputs = Input(shape=(max_seq_length,))\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(inputs)\n",
    "transformer_block_fn = transformer_block(embed_dim=embedding_dim, \n",
    "                                         num_heads=num_heads, \n",
    "                                         ff_dim=ff_dim,\n",
    "                                        dropout_rate=dropout_rate)\n",
    "transformer_block = transformer_block_fn(embedding_layer, training=True)\n",
    "pooling_layer = GlobalAveragePooling1D()(transformer_block)\n",
    "dropout_layer = Dropout(dropout_rate)(pooling_layer)\n",
    "outputs = Dense(units=5, activation='softmax')(dropout_layer)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train, y_train, batch_size=16, epochs=10, validation_data=(x_val, y_val),\n",
    "          callbacks=[early_stopping])\n",
    "\n",
    "# save the trained model\n",
    "model.save('../models/transformer_classification_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f7061ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('../models/transformer_classification_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b739dd",
   "metadata": {},
   "source": [
    "Do more epochs and get better accuracy\n",
    "Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "99ca71ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 2s 17ms/step - loss: 0.7767 - accuracy: 0.6598\n",
      "Validation loss: 0.7767272591590881\n",
      "Validation accuracy: 0.6598475575447083\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on validation data\n",
    "loss, accuracy = model.evaluate(x_val, y_val)\n",
    "print('Validation loss:', loss)\n",
    "print('Validation accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7660750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
