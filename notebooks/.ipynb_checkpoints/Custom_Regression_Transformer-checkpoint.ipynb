{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f2b6f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Layer\n",
    "from keras.layers import Embedding, Input, GlobalAveragePooling1D, Dense\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential, Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b02a2f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_block(embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "    att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "    ffn = Sequential(\n",
    "        [Dense(ff_dim, activation=\"relu\"),\n",
    "         Dense(embed_dim), ]\n",
    "    )\n",
    "    layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "    layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "    dropout1 = Dropout(rate)\n",
    "    dropout2 = Dropout(rate)\n",
    "    \n",
    "    def call(inputs, training):\n",
    "        attn_output = att(inputs, inputs)\n",
    "        attn_output = dropout1(attn_output, training=training)\n",
    "        out1 = layernorm1(inputs + attn_output)\n",
    "        ffn_output = ffn(out1)\n",
    "        ffn_output = dropout2(ffn_output, training=training)\n",
    "        return layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    return call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "585d6f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_and_position_embedding(maxlen, vocab_size, embed_dim):\n",
    "    token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "    pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "    \n",
    "    def call(x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = pos_emb(positions)\n",
    "        x = token_emb(x)\n",
    "        return x + positions\n",
    "    \n",
    "    return call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa52cc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load training data\n",
    "train_df = pd.read_csv('../data/cleaned_train.csv')\n",
    "\n",
    "# Extract the columns you want to use as input features\n",
    "columns = ['BERT_sentiment_score', 'normalised_word_count', 'pub_day', 'pub_hour']\n",
    "# Extract input features from the dataframe\n",
    "x_train = train_df[columns].dropna()\n",
    "\n",
    "y_train = np.array(train_df['n_comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c38a9578",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load test data\n",
    "test_df = pd.read_csv('../data/cleaned_test.csv')\n",
    "\n",
    "x_test = test_df[columns].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4e430dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(raw_data):\n",
    "    # Concatenate train and test data vertically to ensure consistent label encoding\n",
    "    combined_df = pd.concat([train_df['topic'], test_df['topic']], axis=0)\n",
    "\n",
    "    # Create an instance of LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    # Fit and transform the combined data using LabelEncoder\n",
    "    combined_encoded = le.fit_transform(combined_df)\n",
    "    \n",
    "    return np.array(le.transform(raw_data.dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b38736a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the LabelEncoder transformation on train data\n",
    "train_topic_encoded = pd.DataFrame(label_encode(train_df['topic']), columns=['topic encoded'])\n",
    "# Concatenate the 'topic_encoded' tensor with the other input features\n",
    "x_train = pd.concat([x_train, train_topic_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2f4ea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the LabelEncoder transformation on train data\n",
    "test_topic_encoded = pd.DataFrame(label_encode(test_df['topic']), columns=['topic encoded'])\n",
    "# Concatenate the 'topic_encoded' tensor with the other input features\n",
    "x_test = pd.concat([x_test, test_topic_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b467f89f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m ff_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m      8\u001b[0m inputs \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(max_seq_length,))\n\u001b[1;32m----> 9\u001b[0m embedding_layer \u001b[38;5;241m=\u001b[39m \u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m)\u001b[49m(inputs)\n\u001b[0;32m     10\u001b[0m transformer_block_fn \u001b[38;5;241m=\u001b[39m transformer_block(embed_dim\u001b[38;5;241m=\u001b[39membedding_dim, num_heads\u001b[38;5;241m=\u001b[39mnum_heads, ff_dim\u001b[38;5;241m=\u001b[39mff_dim)\n\u001b[0;32m     11\u001b[0m transformer_block \u001b[38;5;241m=\u001b[39m transformer_block_fn(embedding_layer, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\dtensor\\utils.py:95\u001b[0m, in \u001b[0;36mallow_initializer_layout.<locals>._wrap_function\u001b[1;34m(layer_instance, *args, **kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m layout:\n\u001b[0;32m     93\u001b[0m       layout_args[variable_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_layout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m layout\n\u001b[1;32m---> 95\u001b[0m init_method(layer_instance, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Inject the layout parameter after the invocation of __init__()\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layout_param_name, layout \u001b[38;5;129;01min\u001b[39;00m layout_args\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\layers\\core\\embedding.py:129\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[1;34m(self, input_dim, output_dim, embeddings_initializer, embeddings_regularizer, activity_regularizer, embeddings_constraint, mask_zero, input_length, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    128\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_shape\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mNone\u001b[39;00m,)\n\u001b[1;32m--> 129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43minput_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m \u001b[38;5;129;01mor\u001b[39;00m output_dim \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    130\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    131\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBoth `input_dim` and `output_dim` should be positive, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    132\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReceived input_dim = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and output_dim = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m base_layer_utils\u001b[38;5;241m.\u001b[39mv2_dtype_behavior_enabled() \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs):\n\u001b[0;32m    135\u001b[0m   \u001b[38;5;66;03m# In TF1, the dtype defaults to the input dtype which is typically int32,\u001b[39;00m\n\u001b[0;32m    136\u001b[0m   \u001b[38;5;66;03m# so explicitly set it to floatx\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:1527\u001b[0m, in \u001b[0;36mNDFrame.__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__nonzero__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1528\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe truth value of a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is ambiguous. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1529\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse a.empty, a.bool(), a.item(), a.any() or a.all().\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1530\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "# Define model architecture\n",
    "max_seq_length = x_train.shape[1]\n",
    "vocab_size = np.max(x_train) + 1\n",
    "embedding_dim = 32\n",
    "num_heads = 2\n",
    "ff_dim = 32\n",
    "\n",
    "inputs = Input(shape=(max_seq_length,))\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_seq_length)(inputs)\n",
    "transformer_block_fn = transformer_block(embed_dim=embedding_dim, num_heads=num_heads, ff_dim=ff_dim)\n",
    "transformer_block = transformer_block_fn(embedding_layer, training=True)\n",
    "pooling_layer = GlobalAveragePooling1D()(transformer_block)\n",
    "dropout_layer = Dropout(rate=0.1)(pooling_layer)\n",
    "outputs = Dense(units=1, activation='linear')(dropout_layer)  # Change activation to 'linear' for regression\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='mse', metrics=['mae'])  # Change loss function to 'mse' for regression\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=4, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d563f661",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
