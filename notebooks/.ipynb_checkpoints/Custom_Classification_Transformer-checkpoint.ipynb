{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0c76b28",
   "metadata": {},
   "source": [
    "# Classification Transformer Model\n",
    "Finds the tone of an article based on the article's headline, abstract and key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9764e680",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c923e215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Layer\n",
    "from keras.layers import Embedding, Input, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb2ec081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_block(embed_dim, num_heads, ff_dim, dropout_rate):\n",
    "    att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "    ffn = Sequential(\n",
    "        [Dense(ff_dim, activation=\"relu\"),\n",
    "         Dense(embed_dim), ]\n",
    "    )\n",
    "    layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "    layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "    dropout1 = Dropout(dropout_rate)\n",
    "    dropout2 = Dropout(dropout_rate)\n",
    "    \n",
    "    def call(inputs, training):\n",
    "        attn_output = att(inputs, inputs)\n",
    "        attn_output = dropout1(attn_output, training=training)\n",
    "        out1 = layernorm1(inputs + attn_output)\n",
    "        ffn_output = ffn(out1)\n",
    "        ffn_output = dropout2(ffn_output, training=training)\n",
    "        return layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    return call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "61e43b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train_df = pd.read_csv('../data/cleaned_train.csv')\n",
    "x_train = train_df['numerical_sequence'].apply(lambda x: [int(i) for i in x.split(',')])\n",
    "y_train = np.array(train_df['BERT_sentiment_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dce68a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_df = pd.read_csv('../data/cleaned_test.csv')\n",
    "x_test = np.array(test_df['numerical_sequence'].apply(lambda x: [int(i) for i in x.split(',')]))\n",
    "y_test = np.array(test_df['BERT_sentiment_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32b23201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the longest sequence\n",
    "max_seq_length = max([len(seq) for seq in x_test] + [len(seq) for seq in x_train])\n",
    "\n",
    "# Pad sequences with zeros to have the same length\n",
    "x_train = pad_sequences(x_train, maxlen= max_length, padding='post', truncating='post')\n",
    "x_test = pad_sequences(x_test, maxlen= max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2de83b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bba5afcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0143a715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', \n",
    "                               # Number of epochs to wait for improvement\n",
    "                               patience=3,  \n",
    "                               verbose=1, \n",
    "                               # Restore the weights of the best epoch\n",
    "                               restore_best_weights=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3f08237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "# Get the maximum value across all elements in the combined array\n",
    "x_combined = np.concatenate((x_train, x_test, x_val), axis=0)\n",
    "vocab_size = np.max(x_combined) + 1\n",
    "\n",
    "embedding_dim = 16\n",
    "num_heads = 2\n",
    "ff_dim = 32\n",
    "dropout_rate = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "442fe369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 227)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 227, 16)      550928      ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 227, 16)     2160        ['embedding_3[0][0]',            \n",
      " eadAttention)                                                    'embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 227, 16)      0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 227, 16)     0           ['embedding_3[0][0]',            \n",
      " mbda)                                                            'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 227, 16)     32          ['tf.__operators__.add_2[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " sequential_1 (Sequential)      (None, 227, 16)      1072        ['layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 227, 16)      0           ['sequential_1[0][0]']           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 227, 16)     0           ['layer_normalization_2[0][0]',  \n",
      " mbda)                                                            'dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 227, 16)     32          ['tf.__operators__.add_3[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1 (Gl  (None, 16)          0           ['layer_normalization_3[0][0]']  \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 16)           0           ['global_average_pooling1d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 3)            51          ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 554,275\n",
      "Trainable params: 554,275\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define model architecture\n",
    "\n",
    "# Input \n",
    "inputs = Input(shape=(max_seq_length,))\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(inputs)\n",
    "\n",
    "# Hidden\n",
    "transformer_block_fn = transformer_block(embed_dim=embedding_dim, \n",
    "                                         num_heads=num_heads, \n",
    "                                         ff_dim=ff_dim,\n",
    "                                        dropout_rate=dropout_rate)\n",
    "transformer_block = transformer_block_fn(embedding_layer, training=True)\n",
    "pooling_layer = GlobalAveragePooling1D()(transformer_block)\n",
    "dropout_layer = Dropout(dropout_rate)(pooling_layer)\n",
    "\n",
    "# Output\n",
    "outputs = Dense(units=3, activation='softmax')(dropout_layer)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6dd932a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "499/499 [==============================] - 14s 26ms/step - loss: 1.0325 - accuracy: 0.4950 - val_loss: 0.9092 - val_accuracy: 0.6023\n",
      "Epoch 2/10\n",
      "499/499 [==============================] - 13s 25ms/step - loss: 0.7865 - accuracy: 0.6619 - val_loss: 0.8219 - val_accuracy: 0.6404\n",
      "Epoch 3/10\n",
      "499/499 [==============================] - 12s 24ms/step - loss: 0.5089 - accuracy: 0.8084 - val_loss: 0.8870 - val_accuracy: 0.6419\n",
      "Epoch 4/10\n",
      "499/499 [==============================] - 12s 24ms/step - loss: 0.2406 - accuracy: 0.9189 - val_loss: 1.1803 - val_accuracy: 0.6264\n",
      "Epoch 5/10\n",
      "499/499 [==============================] - 13s 26ms/step - loss: 0.0970 - accuracy: 0.9712 - val_loss: 1.4937 - val_accuracy: 0.6103\n",
      "Epoch 6/10\n",
      "498/499 [============================>.] - ETA: 0s - loss: 0.0398 - accuracy: 0.9901Restoring model weights from the end of the best epoch: 3.\n",
      "499/499 [==============================] - 12s 25ms/step - loss: 0.0397 - accuracy: 0.9901 - val_loss: 2.0460 - val_accuracy: 0.6098\n",
      "Epoch 6: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17a99a187c0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train, y_train, batch_size=16, epochs=10, validation_data=(x_val, y_val),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0a313be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained model\n",
    "model.save('../models/transformer_classification_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f7061ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('../models/transformer_classification_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "99ca71ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8872 - accuracy: 0.6424\n",
      "Validation loss: 0.8872432708740234\n",
      "Validation accuracy: 0.6424272656440735\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on validation data\n",
    "loss, accuracy = model.evaluate(x_val, y_val)\n",
    "print('Validation loss:', loss)\n",
    "print('Validation accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7660750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
